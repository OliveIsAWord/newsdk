//
// Lexer.
//

#INCLUDE "<inc>/Lexer.hjk"

// The "engine" of the lexer, the bit that provides the character stream,
// operates by stacking and unstacking source text streams. Each stream has a
// filename and a current line number associated with it, which are ultimately
// used to generate useful diagnostic messages. Streams are associated with
// include files, and also with preprocessor macro expansion.

LexCharTreatment : LexCharBehavior[256] = {
    [0] = CHAR_EOF,

    [' ']  = CHAR_WHITESPACE,
    ['\n'] = CHAR_WHITESPACE,
    ['\t'] = CHAR_WHITESPACE,

    ['^'] = CHAR_SPLIT,
    ['('] = CHAR_SPLIT,
    [')'] = CHAR_SPLIT,
    ['~'] = CHAR_SPLIT,
    [','] = CHAR_SPLIT,
    ['['] = CHAR_SPLIT,
    [']'] = CHAR_SPLIT,
    [':'] = CHAR_SPLIT,
    ['{'] = CHAR_SPLIT,
    ['}'] = CHAR_SPLIT,
    ['#'] = CHAR_SPLIT,

    ['='] = CHAR_COALESCE,
    ['&'] = CHAR_COALESCE,
    ['|'] = CHAR_COALESCE,
    ['!'] = CHAR_COALESCE,
    ['<'] = CHAR_COALESCE,
    ['>'] = CHAR_COALESCE,
    ['+'] = CHAR_COALESCE,
    ['-'] = CHAR_COALESCE,
    ['*'] = CHAR_COALESCE,
    ['/'] = CHAR_COALESCE,
    ['%'] = CHAR_COALESCE,
    ['.'] = CHAR_COALESCE,
    ['@'] = CHAR_COALESCE,
    ['$'] = CHAR_COALESCE,

    // Since CHAR_NORMAL is the first field in the enum, it is guaranteed to
    // be zero in value, so we don't have to explicitly set all the other
    // character behaviors to it, since any non-defined fields here will also
    // be zero.
}

CONST LEX_TOKEN_STRING_SIZE := 128

LexPutbackStack : LexToken[LEX_PUTBACK_STACK_DEPTH]
LexCurrentStream : ^LexStream = NULLPTR
LexFalseCount : ULONG = 0
LexPutbackStackPtr : ULONG = 0
LexSymbolZone : TlZone
LexRootScope : ^TlSymbolTable = NULLPTR
LexCurrentScope : ^TlSymbolTable = NULLPTR

STRUCT LexKeyword
    Entry : TlHashTableEntry, // MUST be at the beginning
    Type : LexTokenType,
    Subtype : LexTokenSubtype,
END

LexKeywordHashTable : TlHashTable
LexInternedStringHashTable : TlHashTable

LexKeywordBumpArray : LexKeyword[128]
LexKeywordBumpIndex := 0

FN LexInsertKeyword (
    IN name : ^UBYTE,
    IN type : LexTokenType,
    IN subtype : LexTokenSubtype,
)

    // Insert the keyword into the keyword hash table.
    // This is done every time the compiler is invoked so it should be VERY
    // fast. To this end we completely avoid dynamic allocation with this silly
    // keyword structure bump allocator. The hash table package does no dynamic
    // allocation, nor does it do any string copies (though it does iterate the
    // string to calculate the hash), so this should all be quite speedy.

    keyword := &LexKeywordBumpArray[LexKeywordBumpIndex]
    LexKeywordBumpIndex += 1

    keyword^.Type = type
    keyword^.Subtype = subtype

    TlInsertHashTable (
        &LexKeywordHashTable, // hashtable
        &keyword^.Entry, // entry
        name, // key
    )
END

FN (TlHashTableEnumeratorF) LexSymbolDelete (
    IN entry : ^TlHashTableEntry,
)

    // TODO: free whatever context symbols have

    TlFreeToZone (
        &LexSymbolZone, // zone
        entry, // block
    )
END

FN LexCreateSymbolTable (
    IN outerscope : ^TlSymbolTable,
) : ^TlSymbolTable

    RETURN TlCreateSymbolTable (
        outerscope, // outerscope
        &LexSymbolDelete, // deletefunc
    )
END

FN LexInitialize ()

    dumpster : UBYTE

    // Create the file block for the source file.

    fileblock := FeCreateFileBlock (
        &FeInputFile[0], // includename
        &dumpster, // OUT created
    )

    FeCopyPathFileBlock (
        fileblock, // fileblock
        &FeInputFile[0], // filepath
    )

    // Initialize the stream zone.

    LexInitializeStreamZone ()

    // Create the root file stream.

    filestream := LexCreateFileStream (
        fileblock, // fileblock
        FeInputFileHandle, // handle
    )

    LexPushStream ( filestream )

    // Create the global symbol table scope.

    LexRootScope = LexCreateSymbolTable ( NULLPTR )

    LexCurrentScope = LexRootScope

    // Initialize the symbol zone.

    TlInitializeZone (
        &LexSymbolZone, // zone
        SIZEOF LexSymbol, // blocksize
    )

    // Initialize interned string hash table.

    TlInitializeHashTable ( &LexInternedStringHashTable )

    // Initialize keyword hash table.

    TlInitializeHashTable ( &LexKeywordHashTable )

    LexInsertKeyword ( "AND", TOKEN_OPER, TOKEN_AND )
    LexInsertKeyword ( "BEGIN", TOKEN_STATEMENT, TOKEN_BEGIN )
    LexInsertKeyword ( "BREAK", TOKEN_STATEMENT, TOKEN_BREAK )
    LexInsertKeyword ( "BYTE", TOKEN_PTYPE, TOKEN_BYTE )
    LexInsertKeyword ( "CAST", TOKEN_OPER, TOKEN_CAST )
    LexInsertKeyword ( "CONST", TOKEN_DECL, TOKEN_CONST )
    LexInsertKeyword ( "CONTINUE", TOKEN_STATEMENT, TOKEN_CONTINUE )
    LexInsertKeyword ( "DO", TOKEN_OTHER, TOKEN_DO )
    LexInsertKeyword ( "ELSE", TOKEN_STATEMENT, TOKEN_ELSE )
    LexInsertKeyword ( "ELSEIF", TOKEN_STATEMENT, TOKEN_ELSEIF )
    LexInsertKeyword ( "END", TOKEN_STATEMENT, TOKEN_END )
    LexInsertKeyword ( "ENUM", TOKEN_DECL, TOKEN_ENUM )
    LexInsertKeyword ( "EXTERN", TOKEN_DECL, TOKEN_EXTERN )
    LexInsertKeyword ( "FALSE", TOKEN_VALUE, TOKEN_FALSE )
    LexInsertKeyword ( "FN", TOKEN_DECL, TOKEN_FN )
    LexInsertKeyword ( "FNPTR", TOKEN_DECL, TOKEN_FNPTR )
    LexInsertKeyword ( "FNSECTION", TOKEN_STATEMENT, TOKEN_FNSECTION )
    LexInsertKeyword ( "GOTO", TOKEN_STATEMENT, TOKEN_GOTO )
    LexInsertKeyword ( "IF", TOKEN_STATEMENT, TOKEN_IF )
    LexInsertKeyword ( "IN", TOKEN_ARGSPEC, TOKEN_IN )
    LexInsertKeyword ( "INT", TOKEN_PTYPE, TOKEN_INT )
    LexInsertKeyword ( "LEAVE", TOKEN_STATEMENT, TOKEN_LEAVE )
    LexInsertKeyword ( "LONG", TOKEN_PTYPE, TOKEN_LONG )
    LexInsertKeyword ( "NOT", TOKEN_OPER, TOKEN_NOT )
    LexInsertKeyword ( "NULL", TOKEN_VALUE, TOKEN_NULL )
    LexInsertKeyword ( "NULLPTR", TOKEN_VALUE, TOKEN_NULLPTR )
    LexInsertKeyword ( "OR", TOKEN_OPER, TOKEN_OR )
    LexInsertKeyword ( "OUT", TOKEN_ARGSPEC, TOKEN_OUT )
    LexInsertKeyword ( "PACKED", TOKEN_OTHER, TOKEN_PACKED )
    LexInsertKeyword ( "PUBLIC", TOKEN_DECL, TOKEN_PUBLIC )
    LexInsertKeyword ( "RETURN", TOKEN_STATEMENT, TOKEN_RETURN )
    LexInsertKeyword ( "SECTION", TOKEN_DECL, TOKEN_SECTION )
    LexInsertKeyword ( "SIZEOF", TOKEN_VALUE, TOKEN_SIZEOF )
    LexInsertKeyword ( "SIZEOFVALUE", TOKEN_OPER, TOKEN_SIZEOFVALUE )
    LexInsertKeyword ( "STRUCT", TOKEN_DECL, TOKEN_STRUCT )
    LexInsertKeyword ( "THEN", TOKEN_OTHER, TOKEN_THEN )
    LexInsertKeyword ( "TO", TOKEN_OTHER, TOKEN_TO )
    LexInsertKeyword ( "TRUE", TOKEN_VALUE, TOKEN_TRUE )
    LexInsertKeyword ( "TYPE", TOKEN_DECL, TOKEN_TYPE )
    LexInsertKeyword ( "UBYTE", TOKEN_PTYPE, TOKEN_UBYTE )
    LexInsertKeyword ( "UINT", TOKEN_PTYPE, TOKEN_UINT )
    LexInsertKeyword ( "ULONG", TOKEN_PTYPE, TOKEN_ULONG )
    LexInsertKeyword ( "UNION", TOKEN_DECL, TOKEN_UNION )
    LexInsertKeyword ( "VOID", TOKEN_PTYPE, TOKEN_VOID )
    LexInsertKeyword ( "WHILE", TOKEN_STATEMENT, TOKEN_WHILE )

    LexInsertKeyword ( "(", TOKEN_LPAREN, 0 )
    LexInsertKeyword ( ")", TOKEN_RPAREN, 0 )
    LexInsertKeyword ( "[", TOKEN_LBRACKET, 0 )
    LexInsertKeyword ( "]", TOKEN_RBRACKET, 0 )
    LexInsertKeyword ( "^", TOKEN_CARET, 0 )
    LexInsertKeyword ( ":", TOKEN_COLON, 0 )
    LexInsertKeyword ( "{", TOKEN_LBRACE, 0 )
    LexInsertKeyword ( "}", TOKEN_RBRACE, 0 )
    LexInsertKeyword ( ",", TOKEN_COMMA, 0 )

    LexInsertKeyword ( "==", TOKEN_OPER, TOKEN_EQUIV )
    LexInsertKeyword ( "!=", TOKEN_OPER, TOKEN_NOTEQUIV )
    LexInsertKeyword ( "&", TOKEN_OPER, TOKEN_BITAND )
    LexInsertKeyword ( "|", TOKEN_OPER, TOKEN_BITOR )
    LexInsertKeyword ( "<", TOKEN_OPER, TOKEN_LESSTHAN )
    LexInsertKeyword ( ">", TOKEN_OPER, TOKEN_GREATERTHAN )
    LexInsertKeyword ( "<=", TOKEN_OPER, TOKEN_LTEQ )
    LexInsertKeyword ( ">=", TOKEN_OPER, TOKEN_GTEQ )
    LexInsertKeyword ( "+", TOKEN_OPER, TOKEN_PLUS )
    LexInsertKeyword ( "-", TOKEN_OPER, TOKEN_MINUS )
    LexInsertKeyword ( "/", TOKEN_OPER, TOKEN_DIVIDE )
    LexInsertKeyword ( "%", TOKEN_OPER, TOKEN_MODULO )
    LexInsertKeyword ( ".", TOKEN_OPER, TOKEN_DOT )
    LexInsertKeyword ( "@", TOKEN_STATEMENT, TOKEN_LABEL )
    LexInsertKeyword ( "$", TOKEN_OPER, TOKEN_BITXOR )
    LexInsertKeyword ( "<<", TOKEN_OPER, TOKEN_LEFTSHIFT )
    LexInsertKeyword ( ">>", TOKEN_OPER, TOKEN_RIGHTSHIFT )
    LexInsertKeyword ( "~", TOKEN_OPER, TOKEN_BITNOT )

    LexInsertKeyword ( "=", TOKEN_ASSIGN, TOKEN_EQUALS )
    LexInsertKeyword ( "+=", TOKEN_ASSIGN, TOKEN_PLUSEQUALS )
    LexInsertKeyword ( "-=", TOKEN_ASSIGN, TOKEN_MINUSEQUALS )
    LexInsertKeyword ( "*=", TOKEN_ASSIGN, TOKEN_MULEQUALS )
    LexInsertKeyword ( "/=", TOKEN_ASSIGN, TOKEN_DIVEQUALS )
    LexInsertKeyword ( "%=", TOKEN_ASSIGN, TOKEN_MODEQUALS )
    LexInsertKeyword ( "&=", TOKEN_ASSIGN, TOKEN_ANDEQUALS )
    LexInsertKeyword ( "|=", TOKEN_ASSIGN, TOKEN_OREQUALS )
    LexInsertKeyword ( "$=", TOKEN_ASSIGN, TOKEN_XOREQUALS )
    LexInsertKeyword ( "<<=", TOKEN_ASSIGN, TOKEN_LSHEQUALS )
    LexInsertKeyword ( ">>=", TOKEN_ASSIGN, TOKEN_RSHEQUALS )

    // Test some stuff (TEMPORARY)

    //TlSummarizeHashTable ( &LexKeywordHashTable )

    buf : UBYTE[LEX_TOKEN_STRING_SIZE]
    len : ULONG

    tok : LexToken

    LexGetToken (
        &tok, // token
    )

    WHILE tok.Type != TOKEN_EOF DO
        //TlPrintNumber ( tok.Type )
        //TlPrintString ( "\n" )

        IF tok.Type == TOKEN_STRING THEN
            str := CAST tok.Payload TO ^LexInternedString

            //TlPrintString ( str^.DynamicBuffer.Buffer )
            //TlPrintString ( "\n" )
        END

        IF tok.Type == TOKEN_NUMBER THEN
            //TlPrintString ( "num: " )
            //TlPrintNumber ( tok.Payload )
            //TlPrintString ( "\n" )
        END

        LexGetToken (
            &tok, // token
        )
    END

    TlPrintNumber ( TlBumpHits )
    TlPrintString ( "\n" )
    TlPrintNumber ( TlHeapHits )
    TlPrintString ( "\n" )
    TlPrintNumber ( TlBumpBytesUsed )
    TlPrintString ( "\n" )

    asmblock := LexAsmBlockListHead

    WHILE asmblock DO
        TlPrintString ( asmblock^.Contents.Buffer )
        TlPrintString ( "\n" )

        asmblock = asmblock^.Next
    END
END

LexCharacterBackslash := FALSE
LexCharacterInString := FALSE
LexCharacterInLiteral := FALSE
LexCharacterPutback : UBYTE = 0
LexCharacterCoalescePutback : UBYTE = 0

FN (LexGetCharacterF) LexGetCharacter () : UBYTE

    // Return the next character in the source stream. Omits comments and text
    // that is killed by the "preprocessor", and notices preprocessor
    // directives, which it ships off to a special little interpreter.

    byte : UBYTE

    comment := FALSE

    WHILE TRUE DO
        IF LexCharacterCoalescePutback THEN
            byte = LexCharacterCoalescePutback
            LexCharacterCoalescePutback = 0

        ELSEIF LexCharacterPutback THEN
            byte = LexCharacterPutback
            LexCharacterPutback = 0

        ELSE
            byte = LexStreamNextCharacter ()

            IF NOT byte THEN
                // EOF has been reached.

                BREAK
            END
        END

        // We care about backslashes and about knowing whether we're in a
        // string or not in this routine, because otherwise we will
        // mis-identify things as comments or directives that aren't.

        IF comment THEN
            IF byte != '\n' THEN
                CONTINUE
            END

            comment = FALSE

            // Make sure this newline is seen by the higher level code,
            // otherwise a token that is immediately followed by a comment
            // with no interfering whitespace may not terminate properly.

        ELSEIF LexCharacterBackslash THEN
            LexCharacterBackslash = FALSE

        ELSEIF byte == '\\' THEN
            LexCharacterBackslash = TRUE

        ELSEIF LexCharacterInString THEN
            LexCharacterInString = NOT (byte == '\"')

        ELSEIF LexCharacterInLiteral THEN
            LexCharacterInLiteral = NOT (byte == 0x27) // single-quote

        ELSEIF byte == '/' THEN
            // We have to figure out if this is a comment or not. This
            // requires looking at the next character and seeing if it is also
            // a forward slash character. If it is, we set comment to true. If
            // it isn't, we pass this forward slash along and set putback to
            // the value of the character we peeked, causing it to be returned
            // on the next call.

            nextbyte := LexStreamNextCharacter ()

            IF nextbyte == '/' THEN
                comment = TRUE

                CONTINUE
            END

            LexCharacterPutback = nextbyte

        ELSEIF byte == '\"' THEN
            LexCharacterInString = TRUE

        ELSEIF byte == 0x27 THEN // single-quote
            LexCharacterInLiteral = TRUE

        ELSEIF byte == '#' THEN
            // We found a preprocessor directive! We dispatch to
            // the preprocessor regardless of the value of the false count.
            // This gives it an opportunity to manipulate the false count, but
            // it must be sure to ignore directives that aren't relevant.

            LexParseDirective ()

            // The preprocessor consumed the newline character terminating the
            // directive, so we return one here for good measure.

            byte = '\n'
        END

        IF LexFalseCount THEN
            // A non-zero false count causes all characters to be ignored
            // until this situation is resolved by a preprocessor directive.
            // This is used for conditional compilation.

            CONTINUE
        END

        BREAK
    END

    RETURN byte
END

CONST TOKEN_FLAG_SPLIT    := 1
CONST TOKEN_FLAG_COALESCE := 2
CONST TOKEN_FLAG_STRING   := 4
CONST TOKEN_FLAG_CHARLIT  := 8
CONST TOKEN_FLAG_UPPER    := 16

FN LexGetTokenInternal (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
    OUT length : ^ULONG,
    OUT internedstring : ^^LexInternedString,
) : UBYTE

    // Label to loop back to in the case that the token turns out to be a macro
    // that needs expansion. This is gross, I know, but the alternative is
    // either a tail call, or wrapping this routine in a giant while true loop,
    // which is grosser.

    @LoopOnMacro

    // Reads the string of the next token into the provided buffer. Identifies
    // macros and expands them. Returns a set of flags indicating things about
    // the token; for instance, if EOF has been reached (i.e. there is no
    // token), if the token is all uppercase, and if the token is a SPLIT or
    // COALESCE token. The buffer is assumed to be LEX_TOKEN_STRING_SIZE bytes
    // in length or greater.

    length^ = 0

    // Skip leading whitespace.

    stream := LexCurrentStream

    IF stream THEN
        token^.FileBlock = stream^.FileBlock
        token^.LineNumber = stream^.LineNumber
        token^.LinePosition = stream^.LinePosition
    END

    byte := LexGetCharacter ()

    WHILE LexCharTreatment[byte] == CHAR_WHITESPACE DO
        // Re-capture the stream location information for each character of
        // whitespace consumed, until we get the real thing.

        stream = LexCurrentStream

        IF stream THEN
            token^.FileBlock = stream^.FileBlock
            token^.LineNumber = stream^.LineNumber
            token^.LinePosition = stream^.LinePosition
        END

        byte = LexGetCharacter ()
    END

    IF NOT byte THEN
        // EOF encountered.

        RETURN 0
    END

    IF byte == '\"' THEN
        // This is a string token.
        // Bump-allocate a LexInternedString.

        status := TlBumpAlloc (
            SIZEOF LexInternedString, // bytes
            internedstring, // OUT ptr
        )

        IF status THEN
            TlInternalError ( "Failed to allocate interned string", 0, 0, 0 )
        END

        dynamicbuffer := &internedstring^^.DynamicBuffer

        TlInitializeDynamicBuffer ( dynamicbuffer )

        LexGetStringTokenInternal (
            token, // token
            '\"', // terminator
            dynamicbuffer, // dynamicbuffer
            NULLPTR, // buffer
            NULLPTR, // OUT length
        )

        // Lookup or insert the string in the interned string hash table.

        str := CAST TlLookupOrInsertHashTable (
            &LexInternedStringHashTable, // hashtable
            CAST internedstring^ TO ^TlHashTableEntry, // entry
            dynamicbuffer^.Buffer, // key
        ) TO ^LexInternedString

        IF str != internedstring^ THEN
            // Release our dynamic buffer and return the one we found.
            // I wish we could free the dynamic buffer structure itself, but
            // we already bump allocated it. This is done under the assumption
            // that the most common case is a unique string, and that we aren't
            // likely to find anything in the hash table.

            TlUninitializeDynamicBuffer ( dynamicbuffer )

            internedstring^ = str
        END

        RETURN TOKEN_FLAG_STRING
    END

    IF byte == 0x27 THEN // single-quote
        // This is a character literal.

        LexGetStringTokenInternal (
            token, // token
            0x27, // terminator
            NULLPTR, // dynamicbuffer
            buffer, // buffer
            length, // OUT length
        )

        RETURN TOKEN_FLAG_CHARLIT
    END

    IF LexCharTreatment[byte] == CHAR_SPLIT THEN
        // This is a split token. It consists of exactly this character and no
        // more.

        length^ = 1
        buffer[0] = byte
        buffer[1] = 0

        RETURN TOKEN_FLAG_SPLIT
    END

    IF LexCharTreatment[byte] == CHAR_COALESCE THEN
        // This is a coalesce token. It consists of this character and the
        // following coalesce characters, and no more.

        len := 0

        WHILE LexCharTreatment[byte] == CHAR_COALESCE DO
            IF len == LEX_TOKEN_STRING_SIZE - 1 THEN
                LexTokenError ( token,
                    "Token length exceeds maximum.", 0, 0, 0 )
            END

            buffer[len] = byte
            len += 1

            byte = LexGetCharacter ()
        END

        buffer[len] = 0
        length^ = len

        IF LexCharTreatment[byte] != CHAR_WHITESPACE THEN
            // Set the putback character to the last one we saw, since that is part
            // of the next token.

            LexCharacterCoalescePutback = byte
        END

        RETURN TOKEN_FLAG_COALESCE
    END

    // This is a normal token. All we have to track is whether it consists only
    // of uppercase letters A-Z, in which case it must be a keyword. It could
    // also turn out to be a macro, in which case we must add it to the stream
    // stack and loop here.

    uppercaseonly := byte >= 'A' AND byte <= 'Z'

    len := 0

    WHILE LexCharTreatment[byte] == CHAR_NORMAL DO
        IF len == LEX_TOKEN_STRING_SIZE - 1 THEN
            LexTokenError ( token, "Token length exceeds maximum.", 0, 0, 0 )
        END

        IF uppercaseonly THEN
            uppercaseonly = byte >= 'A' AND byte <= 'Z'
        END

        buffer[len] = byte
        len += 1

        byte = LexGetCharacter ()
    END

    buffer[len] = 0
    length^ = len

    IF LexCharTreatment[byte] == CHAR_SPLIT OR
        LexCharTreatment[byte] == CHAR_COALESCE THEN

        LexCharacterCoalescePutback = byte
    END

    IF uppercaseonly AND len > 1 THEN
        RETURN TOKEN_FLAG_UPPER
    END

    // Check if this is a defined macro name. If so, place that macro on the top
    // of the stream stack and loop to the beginning of this procedure.
    //
    // N.B. Jackal macros are "hygienic", meaning that they have no access to
    // the "parent scope", except via their arguments, which become special
    // macros that restore the parent scope during their expansion. This might
    // require associating scopes with streams, so that we know what to restore
    // when we pop a stream off the stream stack.
    //
    // NOTE, also, that we might have consumed the whitespace character after
    // the macro, so if the macro's contents don't conclude with a newline, we
    // might not tokenize that correctly. See if the same trick we use to avoid
    // this case with file streams in LexStreamNextCharacter is applicable to
    // macro streams as well.

    macro := CAST TlLookupSymbolTable (
        LexCurrentMacroScope, // symboltable
        &buffer[0], // name
    ) TO ^LexMacro

    IF NOT macro THEN
        // Not a defined macro, just return the token.

        RETURN 0
    END

    LexExpandMacro (
        macro, // macro
        token, // token
    )

    GOTO LoopOnMacro
END

FN LexGetStringTokenInternal (
    IN token : ^LexToken,
    IN terminator : UBYTE,
    IN dynamicbuffer : ^TlDynamicBuffer,
    IN buffer : ^UBYTE,
    OUT length : ^ULONG,
)

    len := 0
    isbackslash := FALSE

    WHILE TRUE DO
        byte := LexGetCharacter ()

        IF NOT byte THEN
            LexTokenError ( token, "String token terminated by EOF!", 0, 0, 0 )
        END

        IF NOT isbackslash THEN
            IF byte == terminator THEN
                // String is done.

                BREAK
            ELSEIF byte == '\\' THEN
                isbackslash = TRUE

                CONTINUE
            END
        ELSE
            isbackslash = FALSE

            IF byte == 'n' THEN
                byte = '\n'
            
            ELSEIF byte == 't' THEN
                byte = '\t'
            
            ELSEIF byte == 'r' THEN
                byte = '\r'
            
            ELSEIF byte == 'b' THEN
                byte = '\b'
            
            ELSEIF byte == '[' THEN
                byte = '\['
            END
        END

        IF dynamicbuffer THEN
            TlInsertDynamicBuffer ( dynamicbuffer, byte )
        
        ELSEIF len == LEX_TOKEN_STRING_SIZE - 1 THEN
            LexTokenError ( token, "Token length exceeds maximum.", 0, 0, 0 )
        
        ELSE
            buffer[len] = byte
            len += 1
        END
    END

    IF dynamicbuffer THEN
        TlInsertDynamicBuffer ( dynamicbuffer, 0 )
    ELSE
        buffer[len] = 0
        length^ = len
    END
END

FN LexGetToken (
    IN token : ^LexToken,
)

    // LexGetTokenInternal might need to allocate an interned string structure.

    internedstring : ^LexInternedString

    buffer : UBYTE[LEX_TOKEN_STRING_SIZE]
    length : ULONG

    tokenflag := LexGetTokenInternal (
        token, // token
        &buffer[0], // buffer
        &length, // OUT length
        &internedstring, // OUT internedstring
    )

    IF tokenflag & ( TOKEN_FLAG_SPLIT | TOKEN_FLAG_COALESCE |
       TOKEN_FLAG_UPPER ) THEN

        // This must be a keyword token. If it doesn't hash to a keyword,
        // there's a syntax error.

        keyword := CAST TlLookupHashTable (
            &LexKeywordHashTable, // hashtable
            &buffer[0], // key
        ) TO ^LexKeyword

        IF NOT keyword THEN
            IF tokenflag & TOKEN_FLAG_UPPER THEN
                LexTokenError ( token,
                    "Bad use of uppercase alphabetic token (must be keyword).",
                    0, 0, 0 )
            END

            LexTokenError ( token, "Unknown keyword", 0, 0, 0 )
        END

        token^.Type = keyword^.Type
        token^.Subtype = keyword^.Subtype

        LEAVE
    END

    IF tokenflag & TOKEN_FLAG_CHARLIT THEN
        // This is a character literal token. We turn those into numerical
        // tokens.

        token^.Type = TOKEN_NUMBER
        token^.Payload = LexCrunchCharacterLiteral ( &buffer[0] )

        LEAVE
    END

    IF tokenflag & TOKEN_FLAG_STRING THEN
        // This is a string token. Record the interned string.

        token^.Type = TOKEN_STRING
        token^.Payload = internedstring

        LEAVE
    END

    IF NOT length THEN
        // EOF has been reached.

        token^.Type = TOKEN_EOF

        LEAVE
    END

    IF buffer[0] >= '0' AND buffer[0] <= '9' THEN
        // This is a numerical token.

        token^.Type = TOKEN_NUMBER
        token^.Payload = LexCrunchNumber (
            token, // token
            &buffer[0], // buffer
        )

        LEAVE
    END

    // This is an identifier. We have to create an entry in the symbol table
    // for the current scope, if one does not exist.

    token^.Type = TOKEN_IDENTIFIER
    token^.Subtype = TOKEN_IDENTIFIER_FOUND

    symbol := CAST TlLookupSymbolTable (
        LexCurrentScope, // symboltable
        &buffer[0], // name
    ) TO ^LexSymbol

    IF NOT symbol THEN
        // Create new symbol. We allocate the symbol from a zone, but allocate
        // the name string from the bump allocator.

        symbol = CAST TlAllocateFromZone ( &LexSymbolZone ) TO ^LexSymbol

        symbolname : ^UBYTE

        status := TlBumpAlloc (
            length + 1, // bytes
            &symbolname, // OUT ptr
        )

        IF status THEN
            TlInternalError ( "Couldn't allocate symbol name", 0, 0, 0 )
        END

        symbol^.Name = symbolname

        TlCopyMemory (
            symbol^.Name, // dest
            &buffer[0], // src
            length + 1, // sz (plus one for null terminator)
        )

        TlInsertSymbolTable (
            LexCurrentScope, // symboltable
            CAST symbol TO ^TlHashTableEntry, // entry
            symbol^.Name, // name
        )

        token^.Subtype = TOKEN_IDENTIFIER_NEW
    END

    token^.Payload = symbol
END

FN LexCrunchCharacterLiteral (
    IN buffer : ^UBYTE,
) : ULONG

    num : ULONG = 0
    i := 0

    WHILE buffer[i] DO
        num *= 256
        num += buffer[i]

        i += 1
    END

    RETURN num
END

FN LexCrunchNumber (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
) : ULONG

    num : ULONG = 0
    i := 0

    IF buffer[0] == '0' THEN
        // This number is either a literal zero, octal, or hexadecimal.

        IF NOT buffer[1] THEN
            // Literal zero.

            RETURN 0
        END

        IF buffer[1] == 'x' THEN
            // Hexadecimal.

            IF NOT buffer[2] THEN
                LexTokenError ( token, "Unfinished hex number.", 0, 0, 0 )
            END

            i = 2

            WHILE buffer[i] DO
                num *= 16

                IF buffer[i] >= '0' AND buffer[i] <= '9' THEN
                    num += buffer[i] - '0'
                
                ELSEIF buffer[i] >= 'A' AND buffer[i] <= 'F' THEN
                    num += buffer[i] - 'A' + 10
                
                ELSEIF buffer[i] >= 'a' AND buffer[i] <= 'f' THEN
                    num += buffer[i] - 'a' + 10

                ELSE
                    LexTokenError ( token, "Malformed hex number.", 0, 0, 0 )
                END

                i += 1
            END

            RETURN num
        END

        // Octal.

        i = 1

        WHILE buffer[i] DO
            IF NOT (buffer[i] >= '0' AND buffer[i] <= '7') THEN
                LexTokenError ( token, "Malformed octal number.", 0, 0, 0 )
            END

            num *= 8
            num += buffer[i] - '0'

            i += 1
        END

        RETURN num
    END

    // Decimal.

    WHILE buffer[i] DO
        IF NOT (buffer[i] >= '0' AND buffer[i] <= '9') THEN
            LexTokenError ( token, "Malformed decimal number.", 0, 0, 0 )
        END

        num *= 10
        num += buffer[i] - '0'

        i += 1
    END

    RETURN num
END