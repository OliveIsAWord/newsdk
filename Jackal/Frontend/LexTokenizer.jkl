//
// The tokenizer of the lexer. Processes the raw character stream into tokens
// for the parser to use. Also does string internment, and creates the symbol
// tables (with scopes directed by the parser).
//

#INCLUDE "<inc>/Lexer.hjk"

LexCharTreatment : LexCharBehavior[256] = {
    [0] = CHAR_EOF,

    [' ']  = CHAR_WHITESPACE,
    ['\n'] = CHAR_WHITESPACE,
    ['\t'] = CHAR_WHITESPACE,

    ['^'] = CHAR_SPLIT,
    ['('] = CHAR_SPLIT,
    [')'] = CHAR_SPLIT,
    ['~'] = CHAR_SPLIT,
    [','] = CHAR_SPLIT,
    ['['] = CHAR_SPLIT,
    [']'] = CHAR_SPLIT,
    [':'] = CHAR_SPLIT,
    ['{'] = CHAR_SPLIT,
    ['}'] = CHAR_SPLIT,
    ['#'] = CHAR_SPLIT,

    ['='] = CHAR_COALESCE,
    ['&'] = CHAR_COALESCE,
    ['|'] = CHAR_COALESCE,
    ['!'] = CHAR_COALESCE,
    ['<'] = CHAR_COALESCE,
    ['>'] = CHAR_COALESCE,
    ['+'] = CHAR_COALESCE,
    ['-'] = CHAR_COALESCE,
    ['*'] = CHAR_COALESCE,
    ['/'] = CHAR_COALESCE,
    ['%'] = CHAR_COALESCE,
    ['.'] = CHAR_COALESCE,
    ['@'] = CHAR_COALESCE,
    ['$'] = CHAR_COALESCE,

    // Since CHAR_NORMAL is the first field in the enum, it is guaranteed to
    // be zero in value, so we don't have to explicitly set all the other
    // character behaviors to it, since any non-defined fields here will also
    // be zero.
}

CONST LEX_TOKEN_STRING_SIZE := 128

LexPutbackStack : LexToken[LEX_PUTBACK_STACK_DEPTH]
LexCurrentStream : ^LexStream = NULLPTR
LexFalseCount : ULONG = 0
LexPutbackStackPtr : ULONG = 0

LexRootScope : ^TlSymbolTable = NULLPTR
LexCurrentScope : ^TlSymbolTable = NULLPTR

LexSymbolZone : TlZone
LexInternedStringZone : TlZone
LexInternedStringHashTable : TlHashTable

STRUCT LexKeyword
    Entry : TlHashTableEntry, // MUST be at the beginning
    Type : LexTokenType,
    Subtype : LexTokenSubtype,
END

LexKeywordHashTable : TlHashTable

LexKeywordBumpArray : LexKeyword[128]
LexKeywordBumpIndex := 0

FN LexInsertKeyword (
    IN name : ^UBYTE,
    IN type : LexTokenType,
    IN subtype : LexTokenSubtype,
)

    // Insert the keyword into the keyword hash table.
    // This is done every time the compiler is invoked so it should be VERY
    // fast. To this end we completely avoid dynamic allocation with this silly
    // keyword structure bump allocator. The hash table package does no dynamic
    // allocation, nor does it do any string copies (though it does iterate the
    // string to calculate the hash), so this should all be quite speedy.

    keyword := &LexKeywordBumpArray[LexKeywordBumpIndex]
    LexKeywordBumpIndex += 1

    keyword^.Type = type
    keyword^.Subtype = subtype

    TlInsertHashTable (
        &LexKeywordHashTable, // hashtable
        &keyword^.Entry, // entry
        name, // key
    )
END

FN (TlHashTableEnumeratorF) LexDeleteSymbol (
    IN entry : ^TlHashTableEntry,
)

    // TODO: free whatever context symbols have

    TlFreeToZone (
        &LexSymbolZone, // zone
        entry, // block
    )
END

FN LexCreateSymbolTable (
    IN outerscope : ^TlSymbolTable,
) : ^TlSymbolTable

    RETURN TlCreateSymbolTable (
        outerscope, // outerscope
        &LexDeleteSymbol, // deletefunc
    )
END

FN LexInitialize ()

    dumpster : UBYTE

    // Create the file block for the source file.

    fileblock := FeCreateFileBlock (
        &FeInputFile[0], // includename
        &dumpster, // OUT created
    )

    FeCopyPathFileBlock (
        fileblock, // fileblock
        &FeInputFile[0], // filepath
    )

    // Initialize the stream zone.

    LexInitializeStreamZone ()

    // Create the root file stream.

    filestream := LexCreateFileStream (
        fileblock, // fileblock
        FeInputFileHandle, // handle
    )

    LexPushStream ( filestream )

    // Create the global symbol table scope.

    LexRootScope = LexCreateSymbolTable ( NULLPTR )

    LexCurrentScope = LexRootScope

    // Initialize the symbol zone.

    TlInitializeZone (
        &LexSymbolZone, // zone
        SIZEOF LexSymbol, // blocksize
    )

    // Initialize the interned string zone.

    TlInitializeZone (
        &LexInternedStringZone, // zone
        SIZEOF LexInternedString, // blocksize
    )

    // Initialize interned string hash table.

    TlInitializeHashTable ( &LexInternedStringHashTable )

    // Initialize keyword hash table.

    TlInitializeHashTable ( &LexKeywordHashTable )

    LexInsertKeyword ( "AND", TOKEN_OPER, TOKEN_AND )
    LexInsertKeyword ( "BEGIN", TOKEN_STATEMENT, TOKEN_BEGIN )
    LexInsertKeyword ( "BREAK", TOKEN_STATEMENT, TOKEN_BREAK )
    LexInsertKeyword ( "BYTE", TOKEN_PTYPE, TOKEN_BYTE )
    LexInsertKeyword ( "CAST", TOKEN_OPER, TOKEN_CAST )
    LexInsertKeyword ( "CONST", TOKEN_DECL, TOKEN_CONST )
    LexInsertKeyword ( "CONTINUE", TOKEN_STATEMENT, TOKEN_CONTINUE )
    LexInsertKeyword ( "DO", TOKEN_OTHER, TOKEN_DO )
    LexInsertKeyword ( "ELSE", TOKEN_STATEMENT, TOKEN_ELSE )
    LexInsertKeyword ( "ELSEIF", TOKEN_STATEMENT, TOKEN_ELSEIF )
    LexInsertKeyword ( "END", TOKEN_STATEMENT, TOKEN_END )
    LexInsertKeyword ( "ENUM", TOKEN_DECL, TOKEN_ENUM )
    LexInsertKeyword ( "EXTERN", TOKEN_DECL, TOKEN_EXTERN )
    LexInsertKeyword ( "FALSE", TOKEN_VALUE, TOKEN_FALSE )
    LexInsertKeyword ( "FN", TOKEN_DECL, TOKEN_FN )
    LexInsertKeyword ( "FNPTR", TOKEN_DECL, TOKEN_FNPTR )
    LexInsertKeyword ( "FNSECTION", TOKEN_STATEMENT, TOKEN_FNSECTION )
    LexInsertKeyword ( "GOTO", TOKEN_STATEMENT, TOKEN_GOTO )
    LexInsertKeyword ( "IF", TOKEN_STATEMENT, TOKEN_IF )
    LexInsertKeyword ( "IN", TOKEN_ARGSPEC, TOKEN_IN )
    LexInsertKeyword ( "INT", TOKEN_PTYPE, TOKEN_INT )
    LexInsertKeyword ( "LEAVE", TOKEN_STATEMENT, TOKEN_LEAVE )
    LexInsertKeyword ( "LONG", TOKEN_PTYPE, TOKEN_LONG )
    LexInsertKeyword ( "NOT", TOKEN_OPER, TOKEN_NOT )
    LexInsertKeyword ( "NULL", TOKEN_VALUE, TOKEN_NULL )
    LexInsertKeyword ( "NULLPTR", TOKEN_VALUE, TOKEN_NULLPTR )
    LexInsertKeyword ( "OR", TOKEN_OPER, TOKEN_OR )
    LexInsertKeyword ( "OUT", TOKEN_ARGSPEC, TOKEN_OUT )
    LexInsertKeyword ( "PACKED", TOKEN_OTHER, TOKEN_PACKED )
    LexInsertKeyword ( "PUBLIC", TOKEN_DECL, TOKEN_PUBLIC )
    LexInsertKeyword ( "RETURN", TOKEN_STATEMENT, TOKEN_RETURN )
    LexInsertKeyword ( "SECTION", TOKEN_DECL, TOKEN_SECTION )
    LexInsertKeyword ( "SIZEOF", TOKEN_VALUE, TOKEN_SIZEOF )
    LexInsertKeyword ( "SIZEOFVALUE", TOKEN_OPER, TOKEN_SIZEOFVALUE )
    LexInsertKeyword ( "STRUCT", TOKEN_DECL, TOKEN_STRUCT )
    LexInsertKeyword ( "THEN", TOKEN_OTHER, TOKEN_THEN )
    LexInsertKeyword ( "TO", TOKEN_OTHER, TOKEN_TO )
    LexInsertKeyword ( "TRUE", TOKEN_VALUE, TOKEN_TRUE )
    LexInsertKeyword ( "TYPE", TOKEN_DECL, TOKEN_TYPE )
    LexInsertKeyword ( "UBYTE", TOKEN_PTYPE, TOKEN_UBYTE )
    LexInsertKeyword ( "UINT", TOKEN_PTYPE, TOKEN_UINT )
    LexInsertKeyword ( "ULONG", TOKEN_PTYPE, TOKEN_ULONG )
    LexInsertKeyword ( "UNION", TOKEN_DECL, TOKEN_UNION )
    LexInsertKeyword ( "VOID", TOKEN_PTYPE, TOKEN_VOID )
    LexInsertKeyword ( "WHILE", TOKEN_STATEMENT, TOKEN_WHILE )

    LexInsertKeyword ( "(", TOKEN_LPAREN, 0 )
    LexInsertKeyword ( ")", TOKEN_RPAREN, 0 )
    LexInsertKeyword ( "[", TOKEN_LBRACKET, 0 )
    LexInsertKeyword ( "]", TOKEN_RBRACKET, 0 )
    LexInsertKeyword ( "^", TOKEN_CARET, 0 )
    LexInsertKeyword ( ":", TOKEN_COLON, 0 )
    LexInsertKeyword ( "{", TOKEN_LBRACE, 0 )
    LexInsertKeyword ( "}", TOKEN_RBRACE, 0 )
    LexInsertKeyword ( ",", TOKEN_COMMA, 0 )

    LexInsertKeyword ( "==", TOKEN_OPER, TOKEN_EQUIV )
    LexInsertKeyword ( "!=", TOKEN_OPER, TOKEN_NOTEQUIV )
    LexInsertKeyword ( "&", TOKEN_OPER, TOKEN_BITAND )
    LexInsertKeyword ( "|", TOKEN_OPER, TOKEN_BITOR )
    LexInsertKeyword ( "<", TOKEN_OPER, TOKEN_LESSTHAN )
    LexInsertKeyword ( ">", TOKEN_OPER, TOKEN_GREATERTHAN )
    LexInsertKeyword ( "<=", TOKEN_OPER, TOKEN_LTEQ )
    LexInsertKeyword ( ">=", TOKEN_OPER, TOKEN_GTEQ )
    LexInsertKeyword ( "+", TOKEN_OPER, TOKEN_PLUS )
    LexInsertKeyword ( "-", TOKEN_OPER, TOKEN_MINUS )
    LexInsertKeyword ( "/", TOKEN_OPER, TOKEN_DIVIDE )
    LexInsertKeyword ( "%", TOKEN_OPER, TOKEN_MODULO )
    LexInsertKeyword ( ".", TOKEN_OPER, TOKEN_DOT )
    LexInsertKeyword ( "@", TOKEN_STATEMENT, TOKEN_LABEL )
    LexInsertKeyword ( "$", TOKEN_OPER, TOKEN_BITXOR )
    LexInsertKeyword ( "<<", TOKEN_OPER, TOKEN_LEFTSHIFT )
    LexInsertKeyword ( ">>", TOKEN_OPER, TOKEN_RIGHTSHIFT )
    LexInsertKeyword ( "~", TOKEN_OPER, TOKEN_BITNOT )

    LexInsertKeyword ( "=", TOKEN_ASSIGN, TOKEN_EQUALS )
    LexInsertKeyword ( "+=", TOKEN_ASSIGN, TOKEN_PLUSEQUALS )
    LexInsertKeyword ( "-=", TOKEN_ASSIGN, TOKEN_MINUSEQUALS )
    LexInsertKeyword ( "*=", TOKEN_ASSIGN, TOKEN_MULEQUALS )
    LexInsertKeyword ( "/=", TOKEN_ASSIGN, TOKEN_DIVEQUALS )
    LexInsertKeyword ( "%=", TOKEN_ASSIGN, TOKEN_MODEQUALS )
    LexInsertKeyword ( "&=", TOKEN_ASSIGN, TOKEN_ANDEQUALS )
    LexInsertKeyword ( "|=", TOKEN_ASSIGN, TOKEN_OREQUALS )
    LexInsertKeyword ( "$=", TOKEN_ASSIGN, TOKEN_XOREQUALS )
    LexInsertKeyword ( "<<=", TOKEN_ASSIGN, TOKEN_LSHEQUALS )
    LexInsertKeyword ( ">>=", TOKEN_ASSIGN, TOKEN_RSHEQUALS )
END

LexCharacterBackslash := FALSE
LexCharacterInString := FALSE
LexCharacterInLiteral := FALSE
LexCharacterPutback : UBYTE = 0
LexCharacterCoalescePutback : UBYTE = 0

FN (LexGetCharacterF) LexGetCharacter () : UBYTE

    // Return the next character in the source stream. Omits comments and text
    // that is killed by the "preprocessor", and notices preprocessor
    // directives, which it ships off to a special little interpreter.

    byte : UBYTE

    comment := FALSE

    WHILE TRUE DO
        IF LexCharacterCoalescePutback THEN
            byte = LexCharacterCoalescePutback
            LexCharacterCoalescePutback = 0

        ELSEIF LexCharacterPutback THEN
            byte = LexCharacterPutback
            LexCharacterPutback = 0

        ELSE
            byte = LexStreamNextCharacter ()

            IF NOT byte THEN
                // EOF has been reached.

                BREAK
            END
        END

        // We care about backslashes and about knowing whether we're in a
        // string or not in this routine, because otherwise we will
        // mis-identify things as comments or directives that aren't.

        IF comment THEN
            IF byte != '\n' THEN
                CONTINUE
            END

            comment = FALSE

            // Make sure this newline is seen by the higher level code,
            // otherwise a token that is immediately followed by a comment
            // with no interfering whitespace may not terminate properly.

        ELSEIF LexCharacterBackslash THEN
            LexCharacterBackslash = FALSE

        ELSEIF byte == '\\' THEN
            LexCharacterBackslash = TRUE

        ELSEIF LexCharacterInString THEN
            LexCharacterInString = NOT (byte == '\"')

        ELSEIF LexCharacterInLiteral THEN
            LexCharacterInLiteral = NOT (byte == 0x27) // single-quote

        ELSEIF byte == '/' THEN
            // We have to figure out if this is a comment or not. This
            // requires looking at the next character and seeing if it is also
            // a forward slash character. If it is, we set comment to true. If
            // it isn't, we pass this forward slash along and set putback to
            // the value of the character we peeked, causing it to be returned
            // on the next call.

            nextbyte := LexStreamNextCharacter ()

            IF nextbyte == '/' THEN
                comment = TRUE

                CONTINUE
            END

            LexCharacterPutback = nextbyte

        ELSEIF byte == '\"' THEN
            LexCharacterInString = TRUE

        ELSEIF byte == 0x27 THEN // single-quote
            LexCharacterInLiteral = TRUE

        ELSEIF byte == '#' THEN
            // We found a preprocessor directive! We dispatch to
            // the preprocessor regardless of the value of the false count.
            // This gives it an opportunity to manipulate the false count, but
            // it must be sure to ignore directives that aren't relevant.

            LexParseDirective ()

            // The preprocessor consumed the newline character terminating the
            // directive, so we return one here for good measure.

            byte = '\n'
        END

        IF LexFalseCount THEN
            // A non-zero false count causes all characters to be ignored
            // until this situation is resolved by a preprocessor directive.
            // This is used for conditional compilation.

            CONTINUE
        END

        BREAK
    END

    RETURN byte
END

CONST TOKEN_FLAG_SPLIT    := 1
CONST TOKEN_FLAG_COALESCE := 2
CONST TOKEN_FLAG_STRING   := 4
CONST TOKEN_FLAG_CHARLIT  := 8
CONST TOKEN_FLAG_UPPER    := 16

FN LexGetTokenContents (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
    OUT length : ^ULONG,
    OUT internedstring : ^^LexInternedString,
) : UBYTE

    // Reads the string of the next token into the provided buffer. Identifies
    // macros and expands them. Returns a set of flags indicating things about
    // the token; for instance, if EOF has been reached (i.e. there is no
    // token), if the token is all uppercase, and if the token is a SPLIT or
    // COALESCE token. The buffer is assumed to be LEX_TOKEN_STRING_SIZE bytes
    // in length or greater.

    // We GOTO LoopOnMacro in the case that the token turns out to be a macro
    // that needs expansion (this is intentionally picked over the alternative
    // of wrapping this routine in a WHILE TRUE loop).

    @LoopOnMacro

    length^ = 0

    // Skip leading whitespace.

    stream := LexCurrentStream

    IF stream THEN
        token^.FileBlock = stream^.FileBlock
        token^.LineNumber = stream^.LineNumber
        token^.LinePosition = stream^.LinePosition
    END

    byte := LexGetCharacter ()

    WHILE LexCharTreatment[byte] == CHAR_WHITESPACE DO
        // Re-capture the stream location information for each character of
        // whitespace consumed, until we get the real thing.

        stream = LexCurrentStream

        IF stream THEN
            token^.FileBlock = stream^.FileBlock
            token^.LineNumber = stream^.LineNumber
            token^.LinePosition = stream^.LinePosition
        END

        byte = LexGetCharacter ()
    END

    IF NOT byte THEN
        // EOF encountered.

        RETURN 0
    END

    IF byte == '\"' THEN
        // This is a string token. Allocate a LexInternedString.

        internedstring^ = CAST TlAllocateFromZone (
            &LexInternedStringZone, // zone
        ) TO ^LexInternedString

        dynamicbuffer := &internedstring^^.DynamicBuffer

        TlInitializeDynamicBuffer ( dynamicbuffer )

        LexGetStringTokenInternal (
            token, // token
            '\"', // terminator
            dynamicbuffer, // dynamicbuffer
            NULLPTR, // buffer
            NULLPTR, // OUT length
        )

        // Lookup or insert the string in the interned string hash table.

        str := CAST TlLookupOrInsertHashTable (
            &LexInternedStringHashTable, // hashtable
            CAST internedstring^ TO ^TlHashTableEntry, // entry
            dynamicbuffer^.Buffer, // key
        ) TO ^LexInternedString

        IF str != internedstring^ THEN
            // Release our dynamic buffer and return the one we found.

            TlUninitializeDynamicBuffer ( dynamicbuffer )

            TlFreeToZone (
                &LexInternedStringZone, // zone
                internedstring^, // block
            )

            internedstring^ = str
        END

        RETURN TOKEN_FLAG_STRING
    END

    IF byte == 0x27 THEN // single-quote
        // This is a character literal.

        LexGetStringTokenInternal (
            token, // token
            0x27, // terminator
            NULLPTR, // dynamicbuffer
            buffer, // buffer
            length, // OUT length
        )

        RETURN TOKEN_FLAG_CHARLIT
    END

    IF LexCharTreatment[byte] == CHAR_SPLIT THEN
        // This is a split token. It consists of exactly this character and no
        // more.

        length^ = 1
        buffer[0] = byte
        buffer[1] = 0

        RETURN TOKEN_FLAG_SPLIT
    END

    IF LexCharTreatment[byte] == CHAR_COALESCE THEN
        // This is a coalesce token. It consists of this character and the
        // following coalesce characters, and no more.

        len := 0

        WHILE LexCharTreatment[byte] == CHAR_COALESCE DO
            IF len == LEX_TOKEN_STRING_SIZE - 1 THEN
                LexTokenError ( token,
                    "Token length exceeds maximum.", 0, 0, 0 )
            END

            buffer[len] = byte
            len += 1

            byte = LexGetCharacter ()
        END

        buffer[len] = 0
        length^ = len

        IF LexCharTreatment[byte] != CHAR_WHITESPACE THEN
            // Set the putback character to the last one we saw, since that is part
            // of the next token.

            LexCharacterCoalescePutback = byte
        END

        RETURN TOKEN_FLAG_COALESCE
    END

    // This is a normal token. All we have to track is whether it consists only
    // of uppercase letters A-Z, in which case it must be a keyword. It could
    // also turn out to be a macro, in which case we must add it to the stream
    // stack and loop here.

    uppercaseonly := byte >= 'A' AND byte <= 'Z'

    len := 0

    WHILE LexCharTreatment[byte] == CHAR_NORMAL DO
        IF len == LEX_TOKEN_STRING_SIZE - 1 THEN
            LexTokenError ( token, "Token length exceeds maximum.", 0, 0, 0 )
        END

        IF uppercaseonly THEN
            uppercaseonly = byte >= 'A' AND byte <= 'Z'
        END

        buffer[len] = byte
        len += 1

        byte = LexGetCharacter ()
    END

    buffer[len] = 0
    length^ = len

    IF LexCharTreatment[byte] == CHAR_SPLIT OR
        LexCharTreatment[byte] == CHAR_COALESCE THEN

        LexCharacterCoalescePutback = byte
    END

    IF uppercaseonly AND len > 1 THEN
        RETURN TOKEN_FLAG_UPPER
    END

    // Check if this is a defined macro name. If so, place that macro on the top
    // of the stream stack and loop to the beginning of this procedure.
    //
    // N.B. Jackal macros are "hygienic", meaning that they have no access to
    // the "parent scope", except via their arguments, which become special
    // macros that restore the parent scope during their expansion. This
    // requires associating scopes with streams, so that we know what to restore
    // when we pop a stream off the stream stack.

    macro := CAST TlLookupSymbolTable (
        LexCurrentMacroScope, // symboltable
        &buffer[0], // name
    ) TO ^LexMacro

    IF NOT macro THEN
        // Not a defined macro, just return the token.

        RETURN 0
    END

    // It is a macro! Expand it and loop, so our caller gets the next "real"
    // token.

    LexExpandMacro (
        macro, // macro
        token, // token
    )

    GOTO LoopOnMacro
END

FN LexGetStringTokenInternal (
    IN token : ^LexToken,
    IN terminator : UBYTE,
    IN dynamicbuffer : ^TlDynamicBuffer,
    IN buffer : ^UBYTE,
    OUT length : ^ULONG,
)

    len := 0
    isbackslash := FALSE

    WHILE TRUE DO
        byte := LexGetCharacter ()

        IF NOT byte THEN
            LexTokenError ( token, "String token terminated by EOF!", 0, 0, 0 )
        END

        IF NOT isbackslash THEN
            IF byte == terminator THEN
                // String is done.

                BREAK
            ELSEIF byte == '\\' THEN
                isbackslash = TRUE

                CONTINUE
            END
        ELSE
            isbackslash = FALSE

            IF byte == 'n' THEN
                byte = '\n'
            
            ELSEIF byte == 't' THEN
                byte = '\t'
            
            ELSEIF byte == 'r' THEN
                byte = '\r'
            
            ELSEIF byte == 'b' THEN
                byte = '\b'
            
            ELSEIF byte == '[' THEN
                byte = '\['
            END
        END

        IF dynamicbuffer THEN
            TlInsertDynamicBuffer ( dynamicbuffer, byte )
        
        ELSEIF len == LEX_TOKEN_STRING_SIZE - 1 THEN
            LexTokenError ( token, "Token length exceeds maximum.", 0, 0, 0 )
        
        ELSE
            buffer[len] = byte
            len += 1
        END
    END

    IF dynamicbuffer THEN
        TlInsertDynamicBuffer ( dynamicbuffer, 0 )
    ELSE
        buffer[len] = 0
        length^ = len
    END
END

FN LexNextToken (
    IN token : ^LexToken,
)

    // Upper level part of token getting.
    // Receives a token string and some flags, and does some extra crunching on
    // it. Ignores the putback stack.

    internedstring : ^LexInternedString

    buffer : UBYTE[LEX_TOKEN_STRING_SIZE]
    length : ULONG

    tokenflag := LexGetTokenContents (
        token, // token
        &buffer[0], // buffer
        &length, // OUT length
        &internedstring, // OUT internedstring
    )

    IF tokenflag & ( TOKEN_FLAG_SPLIT | TOKEN_FLAG_COALESCE |
       TOKEN_FLAG_UPPER ) THEN

        // This must be a keyword token. If it doesn't hash to a keyword,
        // there's a syntax error.

        keyword := CAST TlLookupHashTable (
            &LexKeywordHashTable, // hashtable
            &buffer[0], // key
        ) TO ^LexKeyword

        IF NOT keyword THEN
            IF tokenflag & TOKEN_FLAG_UPPER THEN
                LexTokenError ( token,
                    "Bad use of uppercase alphabetic token (must be keyword).",
                    0, 0, 0 )
            END

            LexTokenError ( token, "Unknown keyword", 0, 0, 0 )
        END

        token^.Type = keyword^.Type
        token^.Subtype = keyword^.Subtype

        LEAVE
    END

    IF tokenflag & TOKEN_FLAG_CHARLIT THEN
        // This is a character literal token. We turn those into numerical
        // tokens.

        token^.Type = TOKEN_NUMBER
        token^.Payload = LexCrunchCharacterLiteral ( &buffer[0] )

        LEAVE
    END

    IF tokenflag & TOKEN_FLAG_STRING THEN
        // This is a string token. Record the interned string.

        token^.Type = TOKEN_STRING
        token^.Payload = internedstring

        LEAVE
    END

    IF NOT length THEN
        // EOF has been reached.

        token^.Type = TOKEN_EOF

        LEAVE
    END

    IF buffer[0] >= '0' AND buffer[0] <= '9' THEN
        // This is a numerical token.

        token^.Type = TOKEN_NUMBER
        token^.Payload = LexCrunchNumber (
            token, // token
            &buffer[0], // buffer
        )

        LEAVE
    END

    // This is an identifier. We have to create an entry in the symbol table
    // for the current scope, if one does not exist.

    token^.Type = TOKEN_IDENTIFIER
    token^.Subtype = TOKEN_IDENTIFIER_FOUND

    symbol := CAST TlLookupSymbolTable (
        LexCurrentScope, // symboltable
        &buffer[0], // name
    ) TO ^LexSymbol

    IF NOT symbol THEN
        // Create new symbol. We allocate the symbol from a zone, but allocate
        // the name string from the bump allocator.

        symbol = CAST TlAllocateFromZone ( &LexSymbolZone ) TO ^LexSymbol

        symbolname : ^UBYTE

        status := TlBumpAlloc (
            length + 1, // bytes
            &symbolname, // OUT ptr
        )

        IF status THEN
            TlInternalError ( "Couldn't allocate symbol name", 0, 0, 0 )
        END

        symbol^.Name = symbolname

        TlCopyMemory (
            symbol^.Name, // dest
            &buffer[0], // src
            length + 1, // sz (plus one for null terminator)
        )

        TlInsertSymbolTable (
            LexCurrentScope, // symboltable
            CAST symbol TO ^TlHashTableEntry, // entry
            symbol^.Name, // name
        )

        token^.Subtype = TOKEN_IDENTIFIER_NEW
    END

    token^.Payload = symbol
END

FN LexCrunchCharacterLiteral (
    IN buffer : ^UBYTE,
) : ULONG

    num : ULONG = 0
    i := 0

    WHILE buffer[i] DO
        num *= 256
        num += buffer[i]

        i += 1
    END

    RETURN num
END

FN LexCrunchNumber (
    IN token : ^LexToken,
    IN buffer : ^UBYTE,
) : TlMachineWord

    num : ULONG = 0
    i := 0

    IF buffer[0] == '0' THEN
        // This number is either a literal zero, octal, or hexadecimal.

        IF NOT buffer[1] THEN
            // Literal zero.

            RETURN 0
        END

        IF buffer[1] == 'x' THEN
            // Hexadecimal.

            IF NOT buffer[2] THEN
                LexTokenError ( token, "Unfinished hex number.", 0, 0, 0 )
            END

            i = 2

            WHILE buffer[i] DO
                num *= 16

                IF buffer[i] >= '0' AND buffer[i] <= '9' THEN
                    num += buffer[i] - '0'
                
                ELSEIF buffer[i] >= 'A' AND buffer[i] <= 'F' THEN
                    num += buffer[i] - 'A' + 10
                
                ELSEIF buffer[i] >= 'a' AND buffer[i] <= 'f' THEN
                    num += buffer[i] - 'a' + 10

                ELSE
                    LexTokenError ( token, "Malformed hex number.", 0, 0, 0 )
                END

                i += 1
            END

            RETURN num
        END

        // Octal.

        i = 1

        WHILE buffer[i] DO
            IF NOT (buffer[i] >= '0' AND buffer[i] <= '7') THEN
                LexTokenError ( token, "Malformed octal number.", 0, 0, 0 )
            END

            num *= 8
            num += buffer[i] - '0'

            i += 1
        END

        RETURN num
    END

    // Decimal.

    WHILE buffer[i] DO
        IF NOT (buffer[i] >= '0' AND buffer[i] <= '9') THEN
            LexTokenError ( token, "Malformed decimal number.", 0, 0, 0 )
        END

        num *= 10
        num += buffer[i] - '0'

        i += 1
    END

    RETURN num
END

FN LexEnterScope ()

    // Enter a new scope.

    LexCurrentScope = TlCreateSymbolTable (
        LexCurrentScope, // outerscope
        &LexDeleteSymbol, // deletefunc
    )
END

FN LexLeaveScope ()

    // Leave the current scope.

    LexCurrentScope = TlDeleteSymbolTable ( LexCurrentScope )
END

FN LexCopyToken (
    IN dest : ^LexToken,
    IN src : ^LexToken,
)

    // Copy all the fields of a source token into a destination token.

    dest^.Payload = src^.Payload
    dest^.FileBlock = src^.FileBlock
    dest^.LineNumber = src^.LineNumber
    dest^.LinePosition = src^.LinePosition
    dest^.Type = src^.Type
    dest^.Subtype = src^.Subtype
END

FN LexGetToken (
    IN token : ^LexToken,
)

    // Read a token from the putback stack if one is present, otherwise read
    // from the source stream.

    index := LexPutbackStackPtr

    IF index THEN
        LexCopyToken (
            token, // dest
            &LexPutbackStack[index - 1], // src
        )

        LexPutbackStackPtr = index - 1

        LEAVE
    END

    LexNextToken ( token )
END

FN LexPutbackToken (
    IN token : ^LexToken,
)

    // Put the token on top of the putback stack.

    index := LexPutbackStackPtr

    IF index == LEX_PUTBACK_STACK_DEPTH THEN
        TlInternalError ( "Lexer putback stack overflow", 0, 0, 0 )
    END

    LexCopyToken (
        &LexPutbackStack[index], // dest
        token, // src
    )

    LexPutbackStackPtr = index + 1
END

FN LexMatchToken (
    IN token : ^LexToken,
    IN type : LexTokenType,
    IN subtype : LexTokenSubtype,
) : UBYTE

    // Read the next token into the caller's token structure, but only if it
    // matches the type and subtype. If the type or subtype are specified as
    // TOKEN_ANY or TOKEN_SUBTYPE_ANY respectively, then it will match any type
    // or subtype of token. Returns TRUE if a token was read, FALSE otherwise.

    index := LexPutbackStackPtr

    IF NOT index THEN
        // The putback stack is empty, read one token directly into the top.

        LexNextToken (
            &LexPutbackStack[0], // token
        )

        LexPutbackStackPtr = 1
        index = 1
    END

    matchtoken := &LexPutbackStack[index - 1]

    IF matchtoken^.Type != type THEN
        // Not a match.

        RETURN FALSE
    END

    IF NOT subtype OR matchtoken^.Subtype == subtype THEN
        // Token matches! Pop it.

        LexCopyToken (
            token, // dest
            matchtoken, // src
        )

        LexPutbackStackPtr = index - 1

        RETURN TRUE
    END

    RETURN FALSE
END